{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od6rKnIv_oxv"
      },
      "source": [
        "# LSTM Sentiment Analysis with Keras\n",
        "How can we take a bunch of movie reviews from IMDB and use code to classify whether or not the review is positive or negative?\n",
        "\n",
        "This notebook goes along with the code in [Learning Intelligence 25].\n",
        "\n",
        "For further reference, check out the [Keras example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJviUgJk_oxx"
      },
      "source": [
        "## What is Sentiment Analysis?\n",
        "\n",
        "Sentiment analysis looks at a body of information and decides whether it's good or bad.\n",
        "\n",
        "For example, IMDB is a movie review site with millions of reviews. However, some reviewers don't leave star ratings. How could IMDB automatically assign star ratings to reviews? Sentiment analysis.\n",
        "\n",
        "What words could you look for in a review which determine whether it's good or bad?\n",
        "\n",
        "Perhaps looking for the word 'great' in a review would lead to a high rating.\n",
        "\n",
        "\"This movie was great, if you think lemon juice in the eyes is great.\"\n",
        "\n",
        "Not so fast. This is one of the hard problems of natural language processing (NLP), taking text or language in its natural form and analysing it.\n",
        "\n",
        "Despite this hard problem, using Keras we can quickly build a model which achieves around 90% accuracy on predicting whether a sentiment is good or bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ae3g9Ga_oxy"
      },
      "source": [
        "## What is Keras?\n",
        "\n",
        "Keras is a deep learning library used to build deep learning models quickly.\n",
        "\n",
        "Keras is based on Python.\n",
        "\n",
        "Because deep learning is a very empircal science (lots of trial and error) Keras is great for building an initial prototype and iterating quickly.\n",
        "\n",
        "See more at [Keras.io](https://keras.io/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xSvLBvj_ox0"
      },
      "source": [
        "## Importing the dependencies\n",
        "\n",
        "Imagine you're starting an assignment. The only way to get information for your assignment is by going to the library and getting books which relate to your project.\n",
        "\n",
        "This is what we're doing here. We need to use a number of things from the Keras library so we're importing (borrowing) them here.\n",
        "\n",
        "What does the dot mean in between `keras.datasets`?\n",
        "\n",
        "Keras is a big library. That dot means we're going into the `datasets` section of the library. Much like you would go to the science section of a regular library.\n",
        "\n",
        "So the statement `from keras.datasets import imdb` is actually saying: Go to the datasets section of the keras library and borrow the imdb dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_14hZAB_ox0",
        "outputId": "1402e57a-da12-4adf-d69d-0a4f93fb4b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported dependencies.\n"
          ]
        }
      ],
      "source": [
        "# Import the dependencies\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, SimpleRNN, Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "print(\"Imported dependencies.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIkqS6PM_ox1"
      },
      "source": [
        "## Setting up the data\n",
        "\n",
        "In deep learning there's often a training set and test set of data.\n",
        "\n",
        "The training set is used for your model to learn on. Essentially, we show our model a bunch of examples and it begins to learn the patterns in those examples.\n",
        "\n",
        "Once it knows the patterns, we can test how accurate those patterns are on the test set (a section of data the model has never seen before).\n",
        "\n",
        "So the line,\n",
        "`(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)` is actually saying:\n",
        "\n",
        "Load the data from the imdb dataset and split it into a training set and a test set and make sure the maximum number of words in each set is 5000.\n",
        "\n",
        "The imdb dataset is a dataset of 25,000 movie reviews built into the Keras library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-0oEMix_ox1",
        "outputId": "0de1cb84-bdba-4620-d139-ef367e715474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Created test and training data.\n"
          ]
        }
      ],
      "source": [
        "# Define the number of words you want to use\n",
        "max_words = 5000\n",
        "\n",
        "# Define the training and test dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "print(\"Created test and training data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n8fRYxL_ox1"
      },
      "source": [
        "## Padding the input sequences\n",
        "\n",
        "Not every review is the same length.\n",
        "\n",
        "Same reviews could even be one word long.\n",
        "\n",
        "\"nice\"\n",
        "\n",
        "Deep learning models look best when all of the data is in a similar shape. Imagine trying to fit 1000 different size marbles through the same size hole.\n",
        "\n",
        "`Pad_sequences` will add 0's to any reviews which don't have a length of 500 (this is what we decided the max length to be, you can increase it).\n",
        "\n",
        "For example, our one word review above would become:\n",
        "\"nice 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0... x 500\"\n",
        "\n",
        "The same goes for any reviews longer than 500 characters, they will be shortened to a maximum of 500.\n",
        "\n",
        "So what `X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)` is saying is: Take the reviews in the X_train dataset and if they are shorter than 500 characters, add 0's to the end and if they're longer than 500 characters, cut them down to 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7iDZW5f_ox1",
        "outputId": "18280475-3b14-45a2-d624-f7dca7110bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded the input sequences with 0's to all be the same length.\n"
          ]
        }
      ],
      "source": [
        "# Define the maximum length of a review\n",
        "max_review_length = 500\n",
        "\n",
        "# Pad the input sequences with 0's to make them all the same length\n",
        "X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "print(\"Padded the input sequences with 0's to all be the same length.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrPDqHYL_ox1"
      },
      "source": [
        "## Creating the model\n",
        "\n",
        "Now our data is ready for some modelling!\n",
        "\n",
        "Deep learning models have layers.\n",
        "\n",
        "The top layer takes in the data we've just prepared, the middle layers do some math on this data and the final layer produces an output we can hopefully make use of.\n",
        "\n",
        "In our case, our model has three layers, an Embedding layer, an LSTM layer and a Dense layer.\n",
        "\n",
        "Our model begins with the line `model = Sequential()`. Think of this as simply stating \"our model will flow from input to output layer in a sequential manner\" or \"our model goes one step at a time\".\n",
        "\n",
        "### Embedding layer\n",
        "\n",
        "The Embedding layer makes creates a database of the relationships between words.\n",
        "\n",
        "`model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))` is saying: add an Embedding layer to our model and use it to turn each of our words into a list of numbers 32 digits long which have some mathematical relationship to each other.\n",
        "\n",
        "So each of our words will become a vector, 32 digits long, of numbers.\n",
        "\n",
        "For example, the = [0.556433, 0.223122, 0.789654....].\n",
        "\n",
        "Don't worry for now how this is computed, Keras does it for us.\n",
        "\n",
        "### RNN layer\n",
        "\n",
        "`model.add(RNN(100))` is saying: add a RNN layer after our embedding layer in our model and give it 100 units.\n",
        "\n",
        "### LSTM layer\n",
        "\n",
        "`model.add(LSTM(100))` is saying: add a LSTM layer after our embedding layer in our model and give it 100 units.\n",
        "\n",
        "LSTM = Long short-term memory. Think of LSTM's as a tap, a tap whichs decides which words flow through the model and which words don't. This layer uses 100 taps to decide which words matter the most in each review.\n",
        "\n",
        "### Dense layer\n",
        "\n",
        "`model.add(Dense(1, activation='sigmoid'))` is saying: add a Dense layer to the end of our model and use a sigmoid activation function to produce a meaningful output.\n",
        "\n",
        "A dense layer is also known as a fully-connected layer. This layer connects the 100 LSTM units in the previous layer to 1 unit. This last unit them takes all this information and runs it through a sigmoid function.\n",
        "\n",
        "Essentially, the sigmoid function will decide if the information should be given a 1 or a -1. 1 for positive and -1 for negative. This is will decided on based on the information passed through by the LSTM layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXvf5ToZ_ox2",
        "outputId": "13616f0e-1dbd-487a-fe26-32294383dd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created.\n"
          ]
        }
      ],
      "source": [
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "#model.add(SimpleRNN(100, return_sequences=True))\n",
        "model.add(SimpleRNN(100))\n",
        "#model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Model created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qT7sqhG_ox2"
      },
      "source": [
        "## Compiling the model\n",
        "\n",
        "The layers of our model our done. But we still have to put some finishing touches on it before it's ready to run.\n",
        "\n",
        "`model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']` is saying: Stack the layers of our model on top of each other and assign a binary crossentropy loss function, use Adam for the optimizer and track accuracy metrics.\n",
        "\n",
        "### Binary crossentropy\n",
        "\n",
        "Think of binary crossentropy as a function which helps decide whether the output of a layer should be a 0 or 1. Binary = 0 or 1. We only want 0 or 1 as the output, because we only care about postive (1) or negative (0). If we cared about more than two categories, we would use a different loss function.\n",
        "\n",
        "### Adam\n",
        "\n",
        "If the model is having a hard time deciding whether an output should be 0 or 1, the Adam optimizer helps out. Adam is the name of a popular optimization function. The optimization function helps the model make better decisions on 0 or 1.\n",
        "\n",
        "### Model metrics\n",
        "\n",
        "Tracking the accuracy metrics will show us some live stats on how our model is doing during training (more on this soon)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R24VEyZK_ox2",
        "outputId": "dd58cf07-c492-44aa-b3af-4f2bb800e20f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ]
        }
      ],
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dynwCCsb_ox2"
      },
      "source": [
        "## Summarize the model\n",
        "\n",
        "Making a summary of the model will give us an idea of what's happening at each layer.\n",
        "\n",
        "In the embedding layer, each of our words is being turned into a list of numbers 32 digits long. Because there are 5000 words (`max_words`), there are 160,000 parameters (32 x 5000).\n",
        "\n",
        "Parameters are individual pieces of information. The goal of the model is to take a large number of parameters and reduce them down to something we can understand and make use of (less parameters).\n",
        "\n",
        "The LSTM layer reduces the number of parameters to 53,200 (5000 x 100 + 32 x 100).\n",
        "\n",
        "The final dense layer connects each of the outputs of the LSTM units into one cell (100 + 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR0E5jy6_ox2",
        "outputId": "459864ea-cae4-4997-fbb0-1ddb7b439f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 32)           160000    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 100)               13300     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 173401 (677.35 KB)\n",
            "Trainable params: 173401 (677.35 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Summarize the different layers in the model\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UodotQm2_ox2"
      },
      "source": [
        "## Fitting the model to the training data\n",
        "\n",
        "Now our model is compiled, it's ready to be set loose on our training data.\n",
        "\n",
        "We're going to run 3 cycles (`epochs=3`) on groups of 64 reviews at a time (`batch_size=64`).\n",
        "\n",
        "Because of our loss and optimzation functions, the model accuracy should improve after each cycle.\n",
        "\n",
        "`model.fit(X_train, y_train, epochs=3, batch_size=64)` is saying: fit the model we've built on the training dataset for 3 cycles and go over 64 reviews at a time.\n",
        "\n",
        "Feel free to change the number of epochs (more cycles) or batch_size (more or less information each step) to see how the accuracy changes.\n",
        "\n",
        "This will take a little time depending on how powerful your computer is. On my MacBook Pro, it took around 10-minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8vSS6uP_ox2",
        "outputId": "d51fd3f1-825a-460d-a519-8bf2046cb1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 77s 194ms/step - loss: 0.6467 - accuracy: 0.5963\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 74s 190ms/step - loss: 0.4933 - accuracy: 0.7626\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 73s 187ms/step - loss: 0.5182 - accuracy: 0.7414\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d3d61fd97e0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft-Ymja1_ox2"
      },
      "source": [
        "## Evaluating the model on the test data\n",
        "\n",
        "The final step is to find out how well our trained model does on the test dataset. The data comes from the same initial library but the model has never seen it.\n",
        "\n",
        "Think of this as studying for an exam. Your teacher tells you the exam will be on things you've learned in class. Training the model is like studying the things you've learned in class. Evaluating the model is like taking the exam.\n",
        "\n",
        "`model_scores = model.evaluate(X_test, y_test, verbose=0)` is saying: take our trained model and see how it performs on the test dataset, we don't want the fancy progress bars so verbose is set to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HEadypM_ox3",
        "outputId": "0e6cffb3-1a8c-4a68-b957-10dfcba2bd70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy on the test dataset: 81.97%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model accuracy on the test dataset: {0:.2f}%\".format(model_scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2d6pbGBGJuQ"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzGr8eOsGJq5",
        "outputId": "cb352b4c-45ee-4923-f208-a5bba7f87bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOoLQSJ1GJok",
        "outputId": "92e71ac7-8a57-49b7-97e8-c1eaea6a8ce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_epoch_4 created.\n"
          ]
        }
      ],
      "source": [
        "# ======= (d) epoch_4 ========\n",
        "\n",
        "\n",
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model_epoch_4 = Sequential()\n",
        "model_epoch_4.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "#model.add(SimpleRNN(100, return_sequences=True))\n",
        "model_epoch_4.add(SimpleRNN(100))\n",
        "#model.add(LSTM(100))\n",
        "model_epoch_4.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Model_epoch_4 created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlEu7jbfJgkT",
        "outputId": "86dbc9ba-b99e-4024-f6a2-293731911c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ]
        }
      ],
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model_epoch_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0UUY5KqJggy",
        "outputId": "cfcf59d2-6772-4436-af8c-ed7e7f1b0df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "391/391 [==============================] - 81s 203ms/step - loss: 0.6179 - accuracy: 0.6297\n",
            "Epoch 2/4\n",
            "391/391 [==============================] - 79s 202ms/step - loss: 0.5380 - accuracy: 0.7380\n",
            "Epoch 3/4\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 0.4233 - accuracy: 0.8137\n",
            "Epoch 4/4\n",
            "391/391 [==============================] - 80s 206ms/step - loss: 0.3673 - accuracy: 0.8420\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d3d630b3c10>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the model to the training data\n",
        "model_epoch_4.fit(X_train, y_train, epochs=4, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y39d_2FjJgep",
        "outputId": "3272827b-be0a-4c98-ae6e-b1b24f19c670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_epoch_4 accuracy on the test dataset: 83.77%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_epoch_4_scores = model_epoch_4.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "model_epoch_4.save('4-(d)_1_layer_RNN100_4epochs . hdf5')\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model_epoch_4 accuracy on the test dataset: {0:.2f}%\".format(model_epoch_4_scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkeoESnTJgcg",
        "outputId": "b0d9cbc6-0a8d-43a2-b3c9-7fe61efd08be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_epoch_5 created.\n"
          ]
        }
      ],
      "source": [
        "# ======= (e) epoch_5 ========\n",
        "\n",
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model_epoch_5 = Sequential()\n",
        "model_epoch_5.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "model_epoch_5.add(SimpleRNN(100, return_sequences=True))\n",
        "model_epoch_5.add(SimpleRNN(100))\n",
        "#model.add(LSTM(100))\n",
        "model_epoch_5.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Model_epoch_5 created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awh7QU47JgaE",
        "outputId": "9a9d1774-be59-4290-a04c-8db5a76414e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ]
        }
      ],
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model_epoch_5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbcz-pZvJgXl",
        "outputId": "fc2a03cc-de3d-4788-d22c-2fad6c44aae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 181s 464ms/step - loss: 0.7015 - accuracy: 0.5326\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 179s 457ms/step - loss: 0.6911 - accuracy: 0.5345\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 180s 462ms/step - loss: 0.6660 - accuracy: 0.5776\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 179s 459ms/step - loss: 0.5695 - accuracy: 0.6990\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 178s 456ms/step - loss: 0.5414 - accuracy: 0.7234\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d3d6561a9b0>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the model to the training data\n",
        "model_epoch_5.fit(X_train, y_train, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QckdNONGJly",
        "outputId": "1294bca1-b6a5-4b27-82ad-24866a275cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_epoch_5 accuracy on the test dataset: 72.44%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_epoch_5_scores = model_epoch_5.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "model_epoch_5.save('4-(d)_2_layer_RNN100_5epochs . hdf5')\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model_RNN50 accuracy on the test dataset: {0:.2f}%\".format(model_epoch_5_scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= (e) -2 epoch_5 with one layer ========\n",
        "\n",
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model_epoch_5_2 = Sequential()\n",
        "model_epoch_5_2.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "model_epoch_5_2.add(SimpleRNN(100))\n",
        "#model.add(LSTM(100))\n",
        "model_epoch_5_2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Model_epoch_5_2 created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRJ6RtpffYXA",
        "outputId": "b0999dfe-ff50-4392-f421-69d3c4a0c42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_epoch_5_2 created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model_epoch_5_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ],
      "metadata": {
        "id": "5cN0YwAhfYQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data\n",
        "model_epoch_5_2.fit(X_train, y_train, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pd0NkYYfYDt",
        "outputId": "6c47e35f-f877-4440-b3be-ea3807b00b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 74s 189ms/step - loss: 0.3362 - accuracy: 0.8581\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.3243 - accuracy: 0.8604\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 84s 216ms/step - loss: 0.4519 - accuracy: 0.7779\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 78s 199ms/step - loss: 0.3455 - accuracy: 0.8546\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 78s 200ms/step - loss: 0.2825 - accuracy: 0.8867\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f9fa4a2af50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_epoch_5_2_scores = model_epoch_5_2.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model_epoch_5_2 accuracy on the test dataset: {0:.2f}%\".format(model_epoch_5_2_scores[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O5CrVTlfX7F",
        "outputId": "db0aab1d-1d31-435f-8b46-0a7c69e5744a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_epoch_5_2 accuracy on the test dataset: 79.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= (e) -3 epoch_3 with two layer ========\n",
        "\n",
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model_epoch_5_3 = Sequential()\n",
        "model_epoch_5_3.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "model_epoch_5_3.add(SimpleRNN(100, return_sequences=True))\n",
        "model_epoch_5_3.add(SimpleRNN(100))\n",
        "#model.add(LSTM(100))\n",
        "model_epoch_5_3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Model_epoch_5_3 created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okDnH8Zhrsqw",
        "outputId": "679ecd30-f967-406a-ebce-9fa4afea7762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_epoch_5_3 created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model_epoch_5_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXXuKGEErsiM",
        "outputId": "c442d992-a31c-44e6-b9d2-cdc1055979c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_epoch_5_3.fit(X_train, y_train, epochs=3, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbPo8dLJrsZf",
        "outputId": "d106ee8e-037f-402d-ccab-67c4ddf97f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 162s 407ms/step - loss: 0.7067 - accuracy: 0.5054\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 160s 409ms/step - loss: 0.6458 - accuracy: 0.6119\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 159s 406ms/step - loss: 0.5586 - accuracy: 0.7050\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f9fa0ccfd90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_epoch_5_3_scores = model_epoch_5_3.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model_epoch_5_2 accuracy on the test dataset: {0:.2f}%\".format(model_epoch_5_3_scores[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6rioLH-ro6v",
        "outputId": "8f3335e8-31f5-49f0-dbb4-64e2e30a1683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_epoch_5_2 accuracy on the test dataset: 63.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V149fi_0GJfI",
        "outputId": "dbf7a7c3-ee14-416b-b7ad-f673cf0db9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_RNN50 created.\n"
          ]
        }
      ],
      "source": [
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model_RNN50 = Sequential()\n",
        "model_RNN50.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "model_RNN50.add(SimpleRNN(50))\n",
        "#model.add(LSTM(100))\n",
        "model_RNN50.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"Model_RNN50 created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbbvs1A_Yha3",
        "outputId": "e481f584-f241-4962-ef36-4edd83773f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ]
        }
      ],
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model_RNN50.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuuK4cPYYhXX",
        "outputId": "21b2b8ef-2e5c-45c7-a80e-c6af34a958da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 59s 147ms/step - loss: 0.6024 - accuracy: 0.6503\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 55s 140ms/step - loss: 0.5947 - accuracy: 0.6917\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 55s 141ms/step - loss: 0.5283 - accuracy: 0.7472\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f9fa9cb6f50>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Fit the model to the training data\n",
        "model_RNN50.fit(X_train, y_train, epochs=3, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjOnnWhZYhVW",
        "outputId": "f254e2c8-bb88-4a46-aa27-bbde03ae5727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_RNN50 accuracy on the test dataset: 75.06%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_RNN50_scores = model_RNN50.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "model_RNN50.save('4-(f)_1_layer_RNN50_3epochs . hdf5')\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model_RNN50 accuracy on the test dataset: {0:.2f}%\".format(model_RNN50_scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU7nUF5IYhTE",
        "outputId": "645955a7-63ea-4875-8eb8-1878a976b1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_RNN50_2 created.\n"
          ]
        }
      ],
      "source": [
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model_RNN50_2 = Sequential()\n",
        "model_RNN50_2.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "model_RNN50_2.add(SimpleRNN(50, return_sequences=True))\n",
        "model_RNN50_2.add(SimpleRNN(50))\n",
        "#model.add(LSTM(100))\n",
        "model_RNN50_2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"model_RNN50_2 created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGC0WOwHYhRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d26c47b-373c-4454-80ae-02946631c4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ]
        }
      ],
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model_RNN50_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mchaJFjqYhOY",
        "outputId": "aef59135-56bc-4983-e46e-fb9c0dec6372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 146s 335ms/step - loss: 0.5560 - accuracy: 0.6938\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 128s 327ms/step - loss: 0.4448 - accuracy: 0.7891\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 127s 324ms/step - loss: 0.4900 - accuracy: 0.7633\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d3d65c15de0>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the model to the training data\n",
        "model_RNN50_2.fit(X_train, y_train, epochs=3, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEKOxVkEGJCe"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_RNN50_2_scores = model_RNN50_2.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "model_RNN50_2.save('4-(g)_2_layer_RNN50_3epochs . hdf5')\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model_RNN50_2 accuracy on the test dataset: {0:.2f}%\".format(model_RNN50_2_scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define how long the embedding vector will be\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# Define the layers in the model\n",
        "model_LSTM = Sequential()\n",
        "model_LSTM.add(Embedding(max_words, embedding_vector_length, input_length=max_review_length))\n",
        "model_LSTM.add(LSTM(100))\n",
        "model_LSTM.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(\"model_LSTM created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGi6BKam7-1f",
        "outputId": "d22b2297-1709-4b23-dd94-346f8b30cf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_LSTM created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model and define the loss and optimization functions\n",
        "model_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled, ready to be fit to the training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft3RzHD88ARc",
        "outputId": "c4721fac-c767-478c-dd47-a9d077692ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled, ready to be fit to the training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the training data\n",
        "model_LSTM.fit(X_train, y_train, epochs=3, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAEnrYEY7-un",
        "outputId": "9c14c068-4548-495d-84c0-b93ff5900bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 275s 696ms/step - loss: 0.4927 - accuracy: 0.7500\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 261s 669ms/step - loss: 0.3498 - accuracy: 0.8496\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 262s 671ms/step - loss: 0.2690 - accuracy: 0.8940\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f9fa0d01cf0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model on the test data\n",
        "model_LSTM_scores = model_LSTM.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "model_LSTM.save('4-(h)_1_layer_LSTM_3epochs . hdf5')\n",
        "\n",
        "# Print out the accuracy of the model on the test set\n",
        "print(\"Model_LSTM accuracy on the test dataset: {0:.2f}%\".format(model_LSTM_scores[1]*100))"
      ],
      "metadata": {
        "id": "gU7RdPkhwlja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b74a5ae-9d84-4e86-dc52-9f29cca2f526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_LSTM accuracy on the test dataset: 86.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sthOO-nQYeiv"
      },
      "source": [
        "## Saving the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUKeYsSCRvKb",
        "outputId": "f5bac106-28e6-41dd-961c-2eb6379beacc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saved\n",
            "loaded\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p checkpoints\n",
        "\n",
        "# save your model\n",
        "model.save('./checkpoints/my_checkpoint')\n",
        "print('saved')\n",
        "\n",
        "# validate your saved model\n",
        "new_model = load_model('./checkpoints/my_checkpoint')\n",
        "print('loaded')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}